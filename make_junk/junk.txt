Below the line of ampersands is the code to a program that lets the user track the level of caffeine in their body, as well as some tests 
to test that code using pytest.

The program maintains three environments: "prod" for production, "tests" for development, and "pytesting" for unit tests.
 
The production and development environments each maintain three I/O files: a log file, a JSON  file with the user's caffeine level at the
last time the code was run, and a second JSON file that lists additions to the user's level to be made in the future.

The pytesting environment maintains on I/O files; the unit tests do not touch the file system.

When desired, the developer can run `rm tests/*.log, tests/*.json` at a Linux command prompt, to remove these files and simulate first run of 
the program.

Thereafter, a call to `python src/caffeine_monitor.py -t 100` should divide `100` into 4 equal parts, write the first part to 
"tests/caff_test.json", and add the remaining three parts to "tests/caff_test_future.json" as a list of dicts.  In addition, if 
there are any levels in "tests/caff_test_future.json" that are now in the past, these values should be removed from 
"caff_test_future.json", decayed, and added to the level in "caff_test.json". Each addition should be logged in "caff_test.log".

A later call to `python src/caffeine_monitor.py -t` should read the most recent level from "caff_test.json", decay this value to 
account for the reduction in caffeine level since the file was last written, read any level values from "caff_test_future.json" 
whose time values are now in the past or in the present, decay these new levels, sum the decayed value from "caff_test.json" with 
the decayed values from "caff_test_future.json", output this sum to the user, and overwrite the contents of "caff_test.json" with 
this same sum. All additions to "caff_test.json" should be logged in "caff_test.log".



We have written a suite of unit tests for this program using Pytest. I'm getting ready to do some refactoring as a next step.

The program was designed to ensure that unit testing and code development do not interfere with the production code and data. 

Some features that were implemented with this goal in mind include:
    - placing input and output files in separate directories, with different file names, for the production, test, and pytesting environments.
    - having the selection of the environment controlled by the value of the CAFF_ENV environment variable
    - having the selection of the environment further controlled by command-line flags
    - making sure that the command-line flags given by the user are correct for the current value of CAFF_ENV
    - ensuring that the unit tests do not touch the file system 

Is it necessary to take futher steps to ensure that these three environments are kept separate? If so, what steps would you recommend?


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

```python
[prod]
json_file = src/caffeine_production.json
json_file_future = src/caffeine_production_future.json
log_file = src/caffeine_production.log

[test]
json_file = tests/caff_test.json
json_file_future = tests/caff_test_future.json
log_file = tests/caff_test.log

[pytesting]
json_file = pytesting/caff_pytesting.json
json_file_future = pytesting/caff_pytesting_future.json
log_file = pytesting/caff_pytesting.log
json_file_scratch = pytesting/caff_pytesting_scratch.json
json_file_future_scratch = pytesting/caff_pytesting_future_scratch.json
log_file_scratch = pytesting/caff_pytesting_scratch.log
```

#====================

```python
#!/usr/bin/env python3.9

# file: src/caffeine_monitor.py
# 2019-10-08


"""
Give a rough estimate of the quantity of caffeine
in the user's body, in mg
"""
from datetime import datetime, timedelta
import json
import logging

from src.utils import set_up


COFFEE_MINS_DECREMENT = 15
SODA_MINS_DECREMENT = 20


class CaffeineMonitor:
    half_life = 360  # in minutes

    def __init__(self, logfile, iofile, iofile_future, first_run, ags):
        """
        # :param logfile: an opened file handle
        # :param iofile: an opened file handle
        # :param iofile_future: an opened file handle
        :param ags: an argparse.Namespace object with .mg as the amount
                    of caffeine consumed, .mins as how long ago the
                    caffeine was consumed, and .bev as the beverage
        """
        self.logfile = logfile
        self.iofile = iofile
        self.iofile_future = iofile_future
        self.data_dict = {}  # data to be read from and dumped to .json file
        self.mg_to_add = int(ags.mg)
        self.mins_ago = int(ags.mins)
        self.mg_net_change = 0.0
        self.beverage = ags.bev
        self.future_list = []
        self.new_future_list = []
        self.log_line_one = ''
        self.first_run = first_run
        self._curr_time = datetime.today()
        self.log_contents = ()

    def main(self):
        """Driver"""
        self.read_log()
        self.read_file()  # sets self.data_dict
        self.read_future_file()  # sets self.future_list
        if not self.first_run:
            self.decay_prev_level()

        if self.beverage == "coffee":
            self.add_coffee()
        elif self.beverage == "soda":
            self.add_soda()
        else:
            pass

        self.process_future_list()

        self.update_time()

        self.write_future_file()

        self.write_file()
        print(self)

    def read_log(self):
        first_line = ''
        last_line = ''
        num_lines = 0
        for log_line in self.logfile:
            num_lines += 1
            if not first_line:
                first_line = log_line.strip()
            last_line = log_line.strip()

        if 0 <= num_lines < 2:
            last_line = ''

        self.log_contents = (first_line, last_line, num_lines)

    def read_file(self):
        """Read initial time and caffeine level from file"""
        self.data_dict = json.load(self.iofile)
        if not self.data_dict:
            self.data_dict = {'time': datetime.now(), 'level': 0.0}

    def read_future_file(self):
        """Read future changes from file"""
        # the sort is redundant; items are sorted before write to file
        self.future_list = sorted(json.load(self.iofile_future), key=lambda x: x['time'], reverse=True)
    
    def write_file(self):
        self.iofile.seek(0)
        self.iofile.truncate(0)
        json.dump(self.data_dict, self.iofile)

    def write_future_file(self):
        self.iofile_future.seek(0)
        self.iofile_future.truncate()
        self.new_future_list.sort(key=lambda x: x['time'], reverse=True)
        json.dump(self.new_future_list, self.iofile_future, indent=4)

    def write_log(self):
        """
        Called by: self.add_caffeine()
        """
        log_mesg = (f'level is {round(self.data_dict["level"], 1)} '
                    f'at {self.data_dict["time"]}')
        if self.mg_net_change:
            log_mesg = (f'{self.mg_net_change:.1f} mg added ({self.mg_to_add} '
                        f'mg, {self.mins_ago} mins ago): ' + log_mesg)
            logging.info(log_mesg)
        else:
            logging.debug(log_mesg)

    def decay_prev_level(self):
        """
        Reduce stored level to account for decay since that value
        was written
        """
        stored_time = datetime.strptime(self.data_dict['time'],
                                        '%Y-%m-%d_%H:%M')
        minutes_elapsed = (self.curr_time -
                           stored_time) / timedelta(minutes=1)
        self.data_dict['time'] = datetime.strftime(self.curr_time,
                                                   '%Y-%m-%d_%H:%M')
        self.data_dict['level'] *= pow(0.5, (minutes_elapsed /
                                             self.half_life))

    def decay_before_add(self, amt_to_decay=None):
        """
        Decay caffeine consumed some time ago
        before it gets added to current level.

        :return: net change rounded to 1 digit past decimal point
        Called by: process_item()
        """
        if amt_to_decay is None:
            amt_to_decay = self.mg_to_add

        # calculate the time at which caffeine was consumed
        old_time = self.curr_time - timedelta(minutes=self.mins_ago)
        minutes_elapsed = (self.curr_time - old_time) / timedelta(minutes=1)
        net_change = (amt_to_decay *
                      pow(0.5, (minutes_elapsed / self.half_life)))
        self.mg_net_change = round(net_change, 1)

    def add_caffeine(self):
        """
        Called by: self.add_coffee()
        """
        if not self.mg_net_change:
            return
        self.data_dict['level'] += self.mg_net_change
        self.write_log()

    def add_coffee(self):
        """
        Called by: main()
        """ 
        quarter = self.mg_to_add / 4
        self.mg_net_change = quarter

        for i in range(4):
            self.process_item()
            self.mins_ago -= COFFEE_MINS_DECREMENT

    def add_soda(self):
        """
        Called by: main()
        """
        # drink 65% now, 25% after SODA_MINS_DECREMENT minutes,
        # remaining 10% after another SODA_MINS_DECREMENT minutes
        soda_amt = self.mg_to_add

        first_amt = soda_amt * 0.65
        self.mg_net_change = first_amt
        # self.mins_ago = 0
        self.process_item()

        second_amt = soda_amt * 0.25
        self.mins_ago -= SODA_MINS_DECREMENT
        self.mg_net_change = second_amt
        self.process_item()

        third_amt = soda_amt * 0.1
        self.mins_ago -= SODA_MINS_DECREMENT
        self.mg_net_change = third_amt
        self.process_item()

    def process_future_list(self):
        """
        Process each item from self.future_list

        Called by: main()
        """
        self.future_list.sort(key=lambda x: x['time'], reverse=True)
        while self.future_list:
            item = self.future_list.pop()
            item_time = datetime.strptime(item['time'], '%Y-%m-%d_%H:%M')
            self.mins_ago = (self.curr_time - item_time) / timedelta(minutes=1)
            self.mg_net_change = item['level']
            self.process_item()
        self.new_future_list.sort(key=lambda x: x['time'], reverse=True)

    def process_item(self):
        """
        Process one caffeine item

        Called by: self.add_coffee(), self.add_soda(), 
                   self.process_future_list()
        """
        if self.mg_net_change == 0:
            return

        if self.mins_ago < 0:  # item is still in the future
            time = (datetime.strptime(self.data_dict['time'], '%Y-%m-%d_%H:%M') +
                    timedelta(minutes=-self.mins_ago))
            new_item = {"time": time.strftime('%Y-%m-%d_%H:%M'), "level": self.mg_net_change}

            # Find the correct position to insert the new item based on its time
            insert_index = 0
            for i, item in enumerate(self.new_future_list):
                if time > datetime.strptime(item['time'], '%Y-%m-%d_%H:%M'):
                    insert_index = i + 1

            # Insert the new item at the correct position
            self.new_future_list.insert(insert_index, new_item)
        elif self.mins_ago == 0:
            self.add_caffeine()
        else:
            self.decay_before_add(self.mg_net_change)
            self.add_caffeine()
            
    def update_time(self):
        """
        Called by: main()
        """
        self.data_dict['time'] = datetime.strftime(datetime.today(),
                                                   '%Y-%m-%d_%H:%M')

    @property
    def curr_time(self):
        return self._curr_time

    @curr_time.setter
    def curr_time(self, new_time):
        self._curr_time = new_time

    def __str__(self):
        return (f'Caffeine level is {round(self.data_dict["level"], 1)} '
                f'mg at time {self.data_dict["time"]}')


if __name__ == '__main__':
    log_filename, json_filename, json_filename_future, first_run, args = set_up()

    try:
        logfile = open(log_filename, 'r+')
    except OSError as e:
        print('Unable to open .log file', e)
        raise
    else:
        with logfile:
            try:
                file = open(json_filename, 'r+')
            except OSError as e:
                print('Unable to open .json file', e)
                raise
            else:
                with file:
                    try:
                        file_future = open(json_filename_future, 'r+')
                    except OSError as e:
                        print('Unable to open future .json file', e)
                        raise
                    else:
                        monitor = CaffeineMonitor(logfile, file, file_future, first_run, args)
                        monitor.main()
```

#====================

```python
# file: src/utils.py
# created: 2020-04-05
import os
import sys
import argparse
import configparser
from datetime import datetime
import json
from pathlib import Path
import logging

CONFIG_FILENAME = 'src/caffeine.ini'


def check_which_environment():
    """
    :return: the current environment ('prod', 'test', or 'pytesting')
    """
    which_env = os.environ.get('CAFF_ENV')
    if which_env is None or which_env not in ('prod', 'test', 'pytesting'):
        print('\nPlease export environment variable CAFF_ENV as '
              'prod, test, or pytesting\n')
        sys.exit(0)
    return which_env


def parse_args(args=None):
    """
    Parse the command line arguments
    :return: an argparse.Namespace instance
    """
    if args is None:
        args = sys.argv[1:]

    # Check for multiple instances of -b/--bev argument
    if args.count('--bev') + args.count('-b') > 1:
        raise ValueError("Duplicate -b/--bev argument")

    parser = argparse.ArgumentParser(description='Estimate the quantity of caffeine (in mg) in the user\'s body')

    env_group = parser.add_mutually_exclusive_group()
    env_group.add_argument('-t', '--test', action='store_true', help='Use test environment')
    env_group.add_argument('-q', '--pytesting', dest='pytesting', action='store_true', help='Use pytesting environment for pytest runs')

    parser.add_argument('mg', nargs='?', type=int, default=0, help='amount of caffeine added (may be 0)')
    parser.add_argument('mins', nargs='?', type=int, default=0, help='minutes ago caffeine was added (may be negative, 0, or omitted)')

    # Parse -b/--bev separately with a default value
    bev_parser = parser.add_argument_group('beverage options')
    bev_parser.add_argument('-b', '--bev', choices=['coffee', 'soda', 'chocolate'], default='coffee', help="beverage: 'coffee' (default), 'soda', or 'chocolate'")

    if '-h' in args or '--help' in args:
        parser.print_help()
        sys.exit(0)

    args = parser.parse_args(args)

    if args.mins < 0:
        print("minutes ago argument (mins) must not be < 0")
        sys.exit(1)

    return args


def create_files(log_filename, json_filename, json_future_filename):
    first_run = False

    my_file = Path(json_filename)
    my_file_future = Path(json_future_filename)

    if not my_file.is_file() or os.path.getsize(my_file) == 0:
        first_run = True
        init_storage(json_filename)
        delete_old_logfile(log_filename)  # if it exists
        init_logfile(log_filename)
        if not my_file_future.is_file():
            init_future(json_future_filename)
    else:
        pass

    return first_run


def read_config_file(config_file):
    conf = configparser.ConfigParser()
    conf.read(config_file)
    return conf


def check_cla_match_env(cur_env, ags):
    """
    Exit with message if the current environment does not match
    the given command line arguments.
    :param cur_env: the current environment ('test', 'prod', or 'pytesting')
    :param ags: an argparse.Namespace object
    :return: None
    """
    ags.test = True if '-t' in sys.argv or '--test' in sys.argv else False
    ags.pytesting = True if '-q' in sys.argv or '--pytesting' in sys.argv else False  # Check for -q or --pytesting flag

    # Note: case `args.test and args.pytesting` handled by `argparse()`

    if ags.pytesting:
        if cur_env != 'pytesting':
            print("Please switch to the pytesting environment with 'export CAFF_ENV=pytesting'")
            sys.exit(0)
    elif ags.test:
        if cur_env != 'test':
            print("Please switch to the test environment with 'export CAFF_ENV=test'")
            sys.exit(0)
    else:  # not ags.test and not ags.pytesting
        if cur_env != 'prod':
            print("You may switch to the production environment with 'export CAFF_ENV=prod'")
            sys.exit(0)


def init_storage(fname):
    """Create a .json file with initial values for time and level"""
    time_now = datetime.strftime(datetime.today(), '%Y-%m-%d_%H:%M')
    start_level = 0
    try:
        with open(fname, 'w') as outfile:
            json.dump({"time": time_now, "level": start_level}, outfile)
    except OSError as er:
        print('Unable to create .json file in `init_storage()`', er)
        raise


def init_future(fname):
    """Create an empty .json file"""
    try:
        with open(fname, 'w') as outfile_future:
            json.dump([], outfile_future)
    except OSError as er:
        print('Unable to create dummy future .json file in `init_future()`',
              er)
        raise


def delete_old_logfile(fname):
    try:
        os.remove(fname)
        return True  # return value only used by testing code at present
    except OSError:
        return False  # ditto


def init_logfile(fname):
    """
    Called by: set_up()
    """
    try:
        with open(fname, 'a+') as logfile:
            print("Start of log file", file=logfile)
    except OSError as er:
        print('Unable to create log file in `init_logfile()`', er)
        raise


def set_up():
    args = parse_args(sys.argv[1:])

    if '-h' in sys.argv[1:] or '--help' in sys.argv[1:]:
        print_help()
        sys.exit(0)

    current_environment = check_which_environment()
    config = read_config_file(CONFIG_FILENAME)

    check_cla_match_env(current_environment, args)

    json_filename = config[current_environment]['json_file']
    json_future_filename = config[current_environment]['json_file_future']
    log_filename = config[current_environment]['log_file']

    first_run = create_files(log_filename, json_filename, json_future_filename)

    logging.basicConfig(filename=config[current_environment]['log_file'],
                        level=logging.INFO,
                        format='%(levelname)s: %(message)s')
    return log_filename, json_filename, json_future_filename, first_run, args
```

#====================

```python
# file: pytesting/conftest.py

import io
import json
from datetime import datetime
import os
import pytest
from pytest_mock import MockerFixture
from caffeine_monitor.src.utils import read_config_file, set_up
from pathlib import Path
import tempfile
from pytesting.temp_file_wrapper import TempFileWrapper
# TODO: introduces coupling to the sut; find another solution
from caffeine_monitor.src.utils import read_config_file, CONFIG_FILENAME


config = read_config_file(CONFIG_FILENAME)
temp_dir = os.path.dirname(config['pytesting']['log_file'])


@pytest.fixture(scope='session')
def temp_json_log_files():
    with tempfile.NamedTemporaryFile(suffix='.log') as log_file, \
         tempfile.NamedTemporaryFile(suffix='.json') as json_file, \
         tempfile.NamedTemporaryFile(suffix='.json', prefix='future_') as json_future_file:
        yield TempFileWrapper(log_file), TempFileWrapper(json_file), TempFileWrapper(json_future_file)


@pytest.fixture(scope='session')
def temp_pytesting_json_log_files():
    with (tempfile.NamedTemporaryFile(suffix='.log') as pytesting_log_file, \
         tempfile.NamedTemporaryFile(suffix='.json') as pytesting_json_file, \
         tempfile.NamedTemporaryFile(suffix='.json', prefix='future_') as pytesting_json_future_file):
        yield (TempFileWrapper(pytesting_log_file), TempFileWrapper(pytesting_json_file),
               TempFileWrapper(pytesting_json_future_file))


@pytest.fixture(scope='session')
def config_ini_mocked(mocker, temp_json_log_files):
    tests_log_file, tests_json_file, tests_json_future_file = temp_json_log_files
    pytesting_log_file, pytesting_json_file, pytesting_json_future_file = temp_json_log_files

    config_mock = {
        'pytesting': {
            'log_file': str(pytesting_log_file.name),
            'json_file': str(pytesting_json_file.name),
            'json_file_future': str(pytesting_json_future_file.name)
        },
        'test': {
            'log_file': str(tests_log_file.tempfile.name),
            'json_file': str(tests_json_file.tempfile.name),
            'json_file_future': str(tests_json_future_file.tempfile.name)
        }
    }

    mocker.patch('caffeine_monitor.src.utils.read_config_file', return_value=config_mock)

    yield


@pytest.fixture
def files_mocked(mocker: MockerFixture):
    open_mock = mocker.patch('builtins.open', new_callable=mocker.mock_open,
                             read_data='Start of log file')

    json_load_mock = mocker.patch('json.load')
    json_dump_mock = mocker.patch('json.dump')

    def json_dump_side_effect(data, file_handle, **kwargs):
        indent = kwargs.get('indent', None)
        if indent is not None:
            file_handle.write(json.dumps(data, indent=indent))
        else:
            file_handle.write(json.dumps(data))

    json_dump_mock.side_effect = json_dump_side_effect

    def json_load_side_effect():
        return {"time": (dt_now := datetime.now().strftime('%Y-%m-%d_%H:%M')), "level": 0.0}

    json_load_mock.side_effect = [
        json_load_side_effect(),
        []
    ]

    open_mock.side_effect = [
        mocker.DEFAULT,  # For the log file
        mocker.DEFAULT,  # For read_file
        mocker.DEFAULT,  # For read_future_file
    ]

    yield open_mock, json_load_mock, json_dump_mock

    json_load_mock.reset_mock(side_effect=True)
    json_dump_mock.reset_mock(side_effect=True)


@pytest.fixture
def files_mocked_with_initial_values(mocker: MockerFixture):
    open_mock = mocker.patch('builtins.open', new_callable=mocker.mock_open,
                             read_data='Start of log file')

    json_load_mock = mocker.patch('json.load')
    json_dump_mock = mocker.patch('json.dump')

    def json_dump_side_effect(data, file_handle, **kwargs):
        indent = kwargs.get('indent', None)
        if indent is not None:
            file_handle.write(json.dumps(data, indent=indent))
        else:
            file_handle.write(json.dumps(data))

    json_dump_mock.side_effect = json_dump_side_effect

    yield open_mock, json_load_mock, json_dump_mock


@pytest.fixture
def mock_file_system(mocker):
    mock_path = mocker.patch('src.utils.Path')
    mock_delete_old_logfile = mocker.patch('src.utils.delete_old_logfile')
    mock_init_logfile = mocker.patch('src.utils.init_logfile')
    mock_init_future = mocker.patch('src.utils.init_future')
    mock_init_storage = mocker.patch('src.utils.init_storage')

    return mock_path, mock_delete_old_logfile, mock_init_logfile, mock_init_future, mock_init_storage
```

#====================

```python
# file: pytesting/temp_file_wrapper.py

class TempFileWrapper:

    def __init__(self, tempfile):
        self.tempfile = tempfile

    def reset(self):
        self.tempfile.seek(0)
        self.tempfile.truncate()
```

#====================

```python
# file: pytesting/test_caffeine_monitor.py

from argparse import Namespace
from datetime import datetime, timedelta, MINYEAR

from freezegun import freeze_time
import json
import pytest
from pytest_mock import MockerFixture
import sys

from caffeine_monitor.src.caffeine_monitor import CaffeineMonitor


def test_can_make_caffeine_monitor_instance_mocked(files_mocked):
    """
    Check CaffeineMonitor ctor makes instance
    """
    open_mock, json_load_mock, json_dump_mock = files_mocked
    nmspc = Namespace(mg=100, mins=180, bev='coffee')
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, True, nmspc)
    assert isinstance(cm_obj, CaffeineMonitor)
    assert cm_obj.mg_to_add == 100
    assert cm_obj.mins_ago == 180


def test_read_file(files_mocked):
    """
    Check read_file() sets data_dict values
    """
    nmspc = Namespace(mg=100, mins=180, bev='coffee')
    open_mock, json_load_mock, json_dump_mock = files_mocked
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, True, nmspc)
    assert(isinstance(cm_obj, CaffeineMonitor))
    assert cm_obj.data_dict == {}
    cm_obj.read_file()
    assert cm_obj.data_dict['level'] == 0.0
    assert cm_obj.data_dict['time'] == datetime.now().strftime('%Y-%m-%d_%H:%M')


def test_write_file_add_mg(files_mocked, caplog):
    """
    Check add_caffeine() adds mg_net_change
    Check add_caffeine() writes correct value to log
    """
    nmspc = Namespace(mg=140, mins=0, bev='coffee')
    open_mock, json_load_mock, json_dump_mock = files_mocked
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, True, nmspc)
    cur_time = datetime.now().strftime('%Y-%m-%d_%H:%M')
    orig_level = 100.0

    cm_obj.data_dict = {'level': orig_level, 'time': cur_time}
    cm_obj.mg_net_change = cm_obj.mg_to_add
    caplog.set_level('INFO')

    cm_obj.add_caffeine()

    assert f'140.0 mg added (140 mg, 0 mins ago): level is {orig_level + cm_obj.mg_net_change} at {cur_time}' in caplog.text
    assert len(caplog.records) == 1


def test_add_no_mg_not_write_log(files_mocked, caplog):
    """
    Check adding 0 mg does not write to log
    """
    nmspc = Namespace(mg=0, mins=0, bev='coffee')
    open_mock, json_load_mock, json_dump_mock = files_mocked
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, True, nmspc)
    cur_time = datetime.now().strftime('%Y-%m-%d_%H:%M')
    cm_obj.data_dict = {'level': 200.0, 'time': cur_time}
    assert cm_obj.mg_to_add == 0.0
    caplog.set_level('DEBUG')

    cm_obj.add_caffeine()

    assert len(caplog.records) == 0


def test_add_no_mg_updates_time(files_mocked):
    """
    Check adding no mg updates the time in caff_test.json
    """
    nmspc = Namespace(mg=0, mins=0, bev='coffee')
    open_mock, json_load_mock, json_dump_mock = files_mocked

    initial_level = 50.0
    json_load_mock.side_effect = [
        {"time": "2020-04-01_12:51", "level": initial_level},
        []
    ]

    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, False, nmspc)
    cur_time = datetime.now().strftime('%Y-%m-%d_%H:%M')
    cm_obj.data_dict = {"time": cur_time, "level": initial_level}

    expected = {"time": cur_time, "level": initial_level}
    cm_obj.write_file()

    json_dump_mock.assert_called_once_with(expected, cm_obj.iofile)


@pytest.mark.parametrize("initial_level, initial_time, time_elapsed, expected_level, mg, mins, bev", [
    (48.0, "2020-04-01_12:51", 360, 24.0, 0, 0, 'coffee'),  # Normal case: 6 hours elapsed
    (100.0, "2020-04-01_12:51", 0, 100.0, 100, 0, 'coffee'),  # Edge case: 0 minutes elapsed
    (300.0, "2020-04-01_12:51", 720, 75.0, 300, 720, 'coffee'),  # Edge case: 12 hours elapsed
    (0.0, "2020-04-01_12:51", 360, 0.0, 0, 360, 'coffee'),  # Edge case: initial level is 0
])
def test_decay_prev_level(files_mocked_with_initial_values, initial_level, initial_time, time_elapsed, expected_level, mg, mins, bev, mocker):
    open_mock, json_load_mock, json_dump_mock = files_mocked_with_initial_values

    # Create a dynamic class to mimic argparse.Namespace
    TestNamespace = type('TestNamespace', (), {'mg': mg, 'mins': mins, 'bev': bev})

    # Create an instance of the dynamic class
    nmspc = TestNamespace()

    # Set the initial data_dict values based on the test case parameters
    json_load_mock.return_value = {"time": initial_time, "level": initial_level}

    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, False, nmspc)
    cm_obj.data_dict = json_load_mock.return_value  # Initialize data_dict

    # Freeze the time before adding the time_elapsed delta
    frozen_time = datetime.strptime(initial_time, '%Y-%m-%d_%H:%M')
    with freeze_time(frozen_time):
        # Set the current time to be time_elapsed minutes later
        cm_obj.curr_time = frozen_time + timedelta(minutes=time_elapsed)
        cm_obj.decay_prev_level()

    # Assert that the level in data_dict is correctly decayed
    assert cm_obj.data_dict['level'] == pytest.approx(expected_level, abs=1e-6)

    # Assert that the time in data_dict is updated correctly
    assert cm_obj.data_dict['time'] == (frozen_time + timedelta(minutes=time_elapsed)).strftime('%Y-%m-%d_%H:%M')


@pytest.mark.parametrize("mg_add, min_ago, net_ch", [
    # Normal cases
    (200, 360, 100.0),
    (100, 180, 70.7),
    (300, 720, 75.0),

    # Edge cases
    (0, 0, 0.0),  # Adding 0 mg
    (200, 0, 200.0),  # Adding caffeine just now (min_ago = 0)
    (200, -60, 224.5),  # Adding caffeine in the future (min_ago < 0)
    (200, 720000, 0.0),  # Adding caffeine a very long time ago (practically decayed to 0)
    (sys.maxsize, 360, sys.maxsize / 2),  # Adding a very large amount of caffeine
])
def test_decay_before_add(files_mocked, mg_add, min_ago, net_ch):
    nmspc = Namespace(mg=mg_add, mins=min_ago, bev='coffee')
    open_mock, json_load_mock, json_dump_mock = files_mocked
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, False, nmspc)
    cm_obj.data_dict = {'level': 0.0, 'time': datetime(2020, 4, 1, 12, 51).strftime('%Y-%m-%d_%H:%M')}
    cm_obj.decay_before_add()
    assert cm_obj.mg_net_change == pytest.approx(net_ch, abs=1e-6)


@pytest.mark.parametrize("mg, mins, expected_mg_net_change, expected_mins_ago", [
    (100, 180, 25, 120),    # Normal case
    (0, 0, 0, -60),         # Edge case: 0 mg and 0 mins
    (200, 0, 50, -60),      # Edge case: mins_ago is 0
    (100, -30, 25, -90),    # Edge case: negative mins_ago
])
def test_add_coffee(mocker, files_mocked, mg, mins, expected_mg_net_change, expected_mins_ago):
    open_mock, json_load_mock, json_dump_mock = files_mocked
    nmspc = Namespace(mg=mg, mins=mins, bev='coffee')
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, True, nmspc)

    mocker.patch.object(cm_obj, 'process_item')

    cm_obj.add_coffee()

    assert cm_obj.mg_net_change == expected_mg_net_change
    assert cm_obj.process_item.call_count == 4
    assert cm_obj.mins_ago == expected_mins_ago


@pytest.mark.parametrize("mg, mins, expected_mg_net_change, expected_mins_ago", [
    (200, 0, 20, -40),      # Normal case
    (0, 0, 0, -40),         # Edge case: 0 mg and 0 mins
    (300, 30, 30, -10),     # Edge case: mins_ago should be 30 - 40 = -10
    (100, -20, 10, -60),    # Edge case: mins_ago should be -20 - 40 = -60
])
def test_add_soda(mocker, files_mocked, mg, mins, expected_mg_net_change, expected_mins_ago):
    open_mock, json_load_mock, json_dump_mock = files_mocked
    nmspc = Namespace(mg=mg, mins=mins, bev='soda')
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, True, nmspc)

    mocker.patch.object(cm_obj, 'process_item')

    cm_obj.add_soda()

    assert cm_obj.mg_net_change == expected_mg_net_change
    assert cm_obj.process_item.call_count == 3
    assert cm_obj.mins_ago == expected_mins_ago


def test_add_caffeine(files_mocked):
    """Test add_caffeine() correctly updates data_dict['level']."""
    open_mock, json_load_mock, json_dump_mock = files_mocked
    nmspc = Namespace(mg=100, mins=0, bev='coffee')
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, True, nmspc)
    cm_obj.data_dict = {"time": "2020-04-01_12:51", "level": 48.0}
    orig_level = cm_obj.data_dict['level']
    cm_obj.mg_net_change = 100.0

    cm_obj.add_caffeine()

    assert cm_obj.data_dict['level'] == orig_level + cm_obj.mg_net_change


def test_update_time(files_mocked):
    nmspc = Namespace(mg=20, mins=20, bev='coffee')
    open_mock, json_load_mock, json_dump_mock = files_mocked
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, True, nmspc)
    cm_obj.read_file()  # loads cm.data_dict from file
    cm_obj.data_dict['time'] = (datetime.now() - timedelta(days=10)).strftime('%Y-%m-%d_%H:%M')
    freezer = freeze_time('2020-05-01 11:00')
    freezer.start()
    cm_obj.update_time()
    freezer.stop()
    assert cm_obj.data_dict['time'] == '2020-05-01_11:00'


def test_str(files_mocked):
    open_mock, json_load_mock, json_dump_mock = files_mocked
    nmspc = Namespace(mg=50, mins=50, bev='soda')
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, False, nmspc)
    cm_obj.data_dict['level'] = 48.0
    cm_obj.data_dict['time'] = datetime(2020, 4, 1, 12, 51).strftime('%Y-%m-%d_%H:%M')
    assert str(cm_obj) == 'Caffeine level is 48.0 mg at time 2020-04-01_12:51'


def test_read_log(files_mocked):
    nmspc = Namespace(mg=100, mins=0, bev='soda')
    open_mock, json_load_mock, json_dump_mock = files_mocked
    # open_mock below must be called; cm_obj.read_log iterates over list
    cm_obj = CaffeineMonitor(open_mock(), json_load_mock, json_load_mock, True, nmspc)
    cm_obj.read_log()
    assert cm_obj.log_contents[0] == 'Start of log file'
    assert cm_obj.log_contents[1] != cm_obj.log_contents[0]
    assert cm_obj.log_contents[2] == 1


def test_read_future_file(files_mocked):
    open_mock, json_load_mock, json_dump_mock = files_mocked

    # Define the expected future list
    expected_future_list = [{"time": "2023-06-08_10:00", "level": 50.0},
                            {"time": "2023-06-08_11:00", "level": 25.0}]

    # Configure the mock file to return the expected future list when json.load() is called
    json_load_mock.side_effect = [expected_future_list]

    # Create an instance of CaffeineMonitor with the mocked files
    nmspc = Namespace(mg=100, mins=180, bev='coffee')
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_load_mock, True, nmspc)

    # Call the read_future_file() method
    cm_obj.read_future_file()

    # Assert that the future_list attribute is set correctly
    assert cm_obj.future_list == sorted(expected_future_list, key=lambda x: x['time'], reverse=True)


def test_write_future_file(files_mocked):
    open_mock, json_load_mock, json_dump_mock = files_mocked

    # Create an instance of CaffeineMonitor with the mocked files
    nmspc = Namespace(mg=100, mins=180, bev='coffee')
    cm_obj = CaffeineMonitor(open_mock, json_load_mock, json_dump_mock, True, nmspc)

    # Set the new_future_list attribute with some test data
    cm_obj.new_future_list = [{"time": "2023-06-08_12:00", "level": 75.0},
                              {"time": "2023-06-08_13:00", "level": 30.0}]

    # Call the write_future_file() method
    cm_obj.write_future_file()

    # Assert that json.dump() is called with the correct arguments
    json_dump_mock.assert_called_once_with(cm_obj.new_future_list, cm_obj.iofile_future, indent=4)


def create_namespace(mg, mins, bev):
    return Namespace(mg=mg, mins=mins, bev=bev)


@pytest.mark.parametrize("mg, mins, bev, first_run", [
    (100, 0, 'coffee', True),
    (50, 30, 'soda', False),
    (0, 0, 'chocolate', True),
    (200, -60, 'coffee', False),
])
@pytest.mark.parametrize("mg_net_change, mins_ago, expected_level, expected_new_future_list", [
    # Normal case: add to current level
    (50.0, 30, pytest.approx(47.2, abs=1e-6), []),

    # Edge cases
    # mg_net_change is 0
    (0.0, 0, 0.0, []),
    # Future item
    (100.0, -60, 0.0, [{"time": (datetime.now() + timedelta(minutes=60)).strftime('%Y-%m-%d_%H:%M'), "level": 100.0}]),
    # Negative mins_ago
    (75.0, -120, 0.0, [{"time": (datetime.now() + timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 75.0}]),
    # mins_ago is a large positive value
    (200.0, 1440, pytest.approx(12.5, abs=1e-6), []),
    # mg_net_change is a very large value
    (sys.maxsize, 360, pytest.approx(sys.maxsize / 2, abs=1e-6), []),
    # mins_ago is a very large negative value
    (100.0, -100000, 0.0, [{"time": (datetime.now() + timedelta(minutes=100000)).strftime('%Y-%m-%d_%H:%M'), "level": 100.0}]),
    # mg_net_change is a negative value
    (-50.0, 60, pytest.approx(-44.5, abs=1e-6), []),
    # mins_ago is a very large positive value (larger than half-life)
    (300.0, 720000, pytest.approx(0.0, abs=1e-6), []),
])
def test_process_item(files_mocked, mg, mins, bev, first_run, mg_net_change, mins_ago, expected_level, expected_new_future_list):
    # Arrange
    ags = create_namespace(mg, mins, bev)
    cm_obj = CaffeineMonitor(*files_mocked, first_run, ags)
    cm_obj.data_dict = {"level": 0.0, "time": datetime.now().strftime('%Y-%m-%d_%H:%M')}
    cm_obj.mg_net_change = mg_net_change
    cm_obj.mins_ago = mins_ago
    cm_obj.new_future_list = []
    cm_obj.half_life = 360  # Setting the half-life to 360 minutes

    # Act
    cm_obj.process_item()

    # Assert
    assert cm_obj.data_dict["level"] == expected_level
    assert cm_obj.new_future_list == expected_new_future_list


@pytest.mark.parametrize("future_list, expected_data_dict_level, expected_new_future_list", [
    # Normal case: process a single past item
    ([{"time": (datetime(2023, 6, 8, 9, 0) - timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 50.0}],
     pytest.approx(39.7, abs=1e-6), []),

    # Normal case: process multiple past items
    ([{"time": (datetime(2023, 6, 8, 9, 0) - timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 50.0},
      {"time": (datetime(2023, 6, 8, 9, 0) - timedelta(minutes=180)).strftime('%Y-%m-%d_%H:%M'), "level": 25.0}],
     pytest.approx(57.4, abs=1e-6), []),

    # Normal case: process a single future item
    ([{"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 50.0}],
     0.0, [{"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 50.0}]),

    # Normal case: process multiple future items
    ([{"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 50.0},
      {"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=180)).strftime('%Y-%m-%d_%H:%M'), "level": 25.0}],
     0.0, [{"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=180)).strftime('%Y-%m-%d_%H:%M'), "level": 25.0},
           {"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 50.0}]),

    # Edge case: empty future list
    ([], 0.0, []),

    # Edge case: process a past item with 0 level
    ([{"time": (datetime(2023, 6, 8, 9, 0) - timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 0.0}],
     0.0, []),

    # Edge case: process a past item with negative level
    ([{"time": (datetime(2023, 6, 8, 9, 0) - timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": -50.0}],
     pytest.approx(-39.7, abs=1e-6), []),

    # Test case: Processing a future list with multiple items that have the same time but different levels, and the time is still in the future at the time the function is called.
    ([{"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 50.0},
      {"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 30.0}],
     0.0, [{"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 50.0},
           {"time": (datetime(2023, 6, 8, 9, 0) + timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 30.0}]),

    # Test case: Processing a future list with multiple items that have the same time but different levels, and the time is in the past at the time the function is called.
    ([{"time": (datetime(2023, 6, 8, 9, 0) - timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 50.0},
      {"time": (datetime(2023, 6, 8, 9, 0) - timedelta(minutes=120)).strftime('%Y-%m-%d_%H:%M'), "level": 30.0}],
     pytest.approx(63.5, abs=1e-1), []),  # TODO: Using pytest.approx with abs=1e-1 as a temporary workaround for the precision issue.

    # Test case: Processing a future list with multiple items that have the same time but different levels, and the time is exactly at the time the function is called.
    ([{"time": datetime(2023, 6, 8, 9, 0).strftime('%Y-%m-%d_%H:%M'), "level": 50.0},
      {"time": datetime(2023, 6, 8, 9, 0).strftime('%Y-%m-%d_%H:%M'), "level": 30.0}],
     80.0, []),
])
def test_process_future_list(files_mocked, future_list, expected_data_dict_level, expected_new_future_list):
    # Arrange
    nmspc = Namespace(mg=0, mins=0, bev='coffee')
    cm_obj = CaffeineMonitor(*files_mocked, True, nmspc)
    cm_obj.data_dict = {"level": 0.0, "time": datetime(2023, 6, 8, 9, 0).strftime('%Y-%m-%d_%H:%M')}
    cm_obj.future_list = future_list
    cm_obj.new_future_list = []
    cm_obj.curr_time = datetime(2023, 6, 8, 9, 0)  # Set a fixed current time

    # Act
    cm_obj.process_future_list()

    # Assert
    assert cm_obj.data_dict["level"] == expected_data_dict_level
    assert cm_obj.new_future_list == expected_new_future_list


class TestCaffeineMonitorMain:
    @pytest.fixture(autouse=True)
    def setup(self, mocker, files_mocked):
        self.mocker = mocker
        self.open_mock, self.json_load_mock, self.json_dump_mock = files_mocked

    @pytest.mark.parametrize("data_dict", [
        {'time': datetime.now().strftime('%Y-%m-%d_%H:%M'), 'level': 0.0},
        {'time': (datetime.now() - timedelta(hours=2)).strftime('%Y-%m-%d_%H:%M'), 'level': 50.0},
        {'time': (datetime.now() + timedelta(hours=1)).strftime('%Y-%m-%d_%H:%M'), 'level': 75.0},
    ])
    def test_main_with_different_data_dict(self, data_dict):
        nmspc = Namespace(mg=100, mins=0, bev='coffee')
        cm_obj = CaffeineMonitor(self.open_mock, self.json_load_mock, self.json_load_mock, True, nmspc)
        self.mocker.patch.object(cm_obj, 'read_file', return_value=None)
        cm_obj.data_dict = data_dict  # Populate data_dict directly
        self.mock_main_sub_methods(cm_obj)
        cm_obj.main()
        self.assert_main_sub_methods_called(cm_obj, first_run=True)

    @pytest.mark.parametrize("mg, mins, bev, first_run", [
        (100, 0, 'coffee', True),
        (50, 30, 'soda', False),
        (0, 0, 'chocolate', True),
        (200, -60, 'coffee', False),
    ])
    def test_main_with_different_params(self, mg, mins, bev, first_run):
        nmspc = Namespace(mg=mg, mins=mins, bev=bev)
        cm_obj = CaffeineMonitor(self.open_mock, self.json_load_mock, self.json_load_mock, first_run, nmspc)
        self.mocker.patch.object(cm_obj, 'read_file', return_value=None)
        cm_obj.data_dict = {'time': datetime.now().strftime('%Y-%m-%d_%H:%M'), 'level': 0.0}  # Populate data_dict
        self.mock_main_sub_methods(cm_obj)
        cm_obj.main()
        self.assert_main_sub_methods_called(cm_obj, first_run=first_run, bev=bev)

    def mock_main_sub_methods(self, cm_obj):
        self.read_log_mock = self.mocker.patch.object(cm_obj, 'read_log')
        self.read_file_mock = self.mocker.patch.object(cm_obj, 'read_file')
        self.read_future_file_mock = self.mocker.patch.object(cm_obj, 'read_future_file')
        self.decay_prev_level_mock = self.mocker.patch.object(cm_obj, 'decay_prev_level')
        self.add_coffee_mock = self.mocker.patch.object(cm_obj, 'add_coffee')
        self.add_soda_mock = self.mocker.patch.object(cm_obj, 'add_soda')
        self.process_future_list_mock = self.mocker.patch.object(cm_obj, 'process_future_list')
        self.update_time_mock = self.mocker.patch.object(cm_obj, 'update_time')
        self.write_future_file_mock = self.mocker.patch.object(cm_obj, 'write_future_file')
        self.write_file_mock = self.mocker.patch.object(cm_obj, 'write_file')

    def assert_main_sub_methods_called(self, cm_obj, first_run, bev=None):
        self.read_log_mock.assert_called_once()
        self.read_file_mock.assert_called_once()
        self.read_future_file_mock.assert_called_once()
        if not first_run:
            self.decay_prev_level_mock.assert_called_once()
        else:
            self.decay_prev_level_mock.assert_not_called()
        if cm_obj.beverage == 'coffee':
            self.add_coffee_mock.assert_called_once()
            self.add_soda_mock.assert_not_called()
        elif cm_obj.beverage == 'soda':
            self.add_soda_mock.assert_called_once()
            self.add_coffee_mock.assert_not_called()
        else:
            self.add_coffee_mock.assert_not_called()
            self.add_soda_mock.assert_not_called()
```

#====================

```python
# file: pytesting/test_utils.py

from argparse import Namespace
import src.utils
import sys
import os
import json
import pytest
from pytest_mock import mocker
from freezegun import freeze_time

from src.utils import (check_which_environment, parse_args,
                       read_config_file, check_cla_match_env, init_storage,
                       delete_old_logfile, create_files, init_future, init_logfile,
                       set_up)
import subprocess
from src.caffeine_monitor import CaffeineMonitor
import builtins
import logging


def test_bad_caff_env_value_exits(mocker):
    mocker.patch('os.environ')
    mocker.patch('sys.exit')
    os.environ['CAFF_ENV'] = 'bongo'
    __ = check_which_environment()
    assert sys.exit.called_once_with(0)


@pytest.mark.parametrize(
    "args, expected",
    [
        (["200"], {"mg": 200}),
        (["200", "360"], {"mg": 200, "mins": 360}),
        (["0", "0", "--bev", "soda"], {"mg": 0, "mins": 0, "bev": "soda"}),
        (["100", "-b", "chocolate"], {"mg": 100, "mins": 0, "bev": "chocolate"}),
        (["-b", "coffee"], {"mg": 0, "mins": 0, "bev": "coffee"}),
        (["100", "20", "--bev", "invalid"], SystemExit),
        (["-h"], SystemExit),
        (["abc"], SystemExit),  # Invalid type for mg
        (["100", "abd"], SystemExit),  # Invalid type for mins
        (["100", "-60"], SystemExit),  # Negative value for mins
        (["100", "20", "--bev", "whiskey"], SystemExit),  # Invalid beverage type
        (["100", "-b"], SystemExit),  # Missing beverage type after -b
    ],
)
def test_parse_args(args, expected):
    if expected == SystemExit:
        with pytest.raises(SystemExit):
            parse_args(args)
    else:
        parsed_args = parse_args(args)
        for key, value in expected.items():
            assert getattr(parsed_args, key) == value


@pytest.mark.parametrize("env, expected_output", [
    (None, "Please export environment variable CAFF_ENV as"),
    ("pytesting", "pytesting"),
    ("prod", "prod"),
    ("test", "test"),
])
def test_check_which_environment(mocker, capsys, env, expected_output):
    if env is None:
        mocker.patch.dict("os.environ", {}, clear=True)
    else:
        mocker.patch.dict("os.environ", {"CAFF_ENV": env})

    if env is None:
        with pytest.raises(SystemExit):
            check_which_environment()
        out, _ = capsys.readouterr()
        assert expected_output in out
    else:
        assert check_which_environment() == expected_output


def test_read_config_file_fake(tmpdir):
    fh = tmpdir.join("config.ini")
    fh.write('''\
[prod]
json_file = src/caffeine_production.json
log_file = src/caffeine_production.log

[test]
json_file = tests/caff_test.json
log_file = tests/caff_test.log
    ''')
    config = read_config_file(fh)
    assert config.sections() == ['prod', 'test']
    assert config['prod'], {'json_file': 'src/caffeine_production.json',
                            'log_file': 'src/caffeine_production.log'}
    assert config['test'], {'json_file': 'tests/caff_test.json',
                            'log_file': 'tests/caff_test.log'}


@pytest.mark.parametrize(
    "cur_env, test_flag, pytesting_flag, expected_exit",
    [
        ("pytesting", False, True, False),
        ("pytesting", True, False, True),
        ("pytesting", False, False, True),
        ("test", False, True, True),
        ("test", True, False, False),
        ("test", False, False, True),
        ("prod", False, True, True),
        ("prod", True, False, True),
        ("prod", False, False, False),
    ],
)
def test_check_cla_match_env(mocker, cur_env, test_flag, pytesting_flag, expected_exit):
    args = mocker.MagicMock()
    args.test = test_flag
    args.pytesting = pytesting_flag

    mocker.patch("sys.argv", ["script.py"])
    if test_flag:
        mocker.patch("sys.argv", ["script.py", "-t"])
    elif pytesting_flag:
        mocker.patch("sys.argv", ["script.py", "-q"])

    mock_exit = mocker.patch("sys.exit")

    check_cla_match_env(cur_env, args)

    if expected_exit:
        mock_exit.assert_called_once_with(0)
    else:
        mock_exit.assert_not_called()


def test_init_storage_bad_filename_raises_oserror(mocker):
    # Patch the open function with our mocked open object
    mock_open = mocker.patch.object(builtins, 'open', side_effect=OSError('Mock OSError'))

    # Call the init_storage function, which should now raise an OSError
    with pytest.raises(OSError):
        init_storage('a/b')

    # Assert that the mocked open function was called with the invalid filename
    mock_open.assert_called_with('a/b', 'w')


def test_init_storage_stores_good_json_file(tmpdir):
    filename = tmpdir.join('delete_me.json')
    freezer = freeze_time('2020-03-26 14:13')
    freezer.start()
    init_storage(filename)
    freezer.stop()
    with open(filename) as file_handle:
        line_read = json.load(file_handle)
        assert line_read == {'time': '2020-03-26_14:13', 'level': 0}
        file_handle.close()
    os.remove(filename)


def test_delete_old_logfile_success(tmpdir):
    filename = tmpdir.join('bogus.log')
    with open(filename, 'w') as handle:
        handle.close()
    assert delete_old_logfile(filename)


def test_delete_old_logfile_failure(tmpdir):
    name_string = 'bogus.log'
    filename = tmpdir.join(name_string)
    while os.path.isfile(filename):  # make *sure* file doesn't exist
        name_string += 'x'
        filename = tmpdir.join(name_string)
    assert not delete_old_logfile(filename)


def arg_helper(*L):
    assert len(L) in [2, 3]
    a, b, c = L
    assert isinstance(a, int)
    assert isinstance(b, int)
    assert isinstance(c, str)


def test_create_files_first_run(mock_file_system):
    # Arrange
    log_filename = 'test.log'
    json_filename = 'test.json'
    json_future_filename = 'test_future.json'

    mock_path, mock_delete_old_logfile, mock_init_logfile, mock_init_future, mock_init_storage = mock_file_system
    mock_path.return_value.is_file.side_effect = [False, False]

    # Act
    first_run = create_files(log_filename, json_filename, json_future_filename)

    # Assert
    assert first_run == True
    mock_init_storage.assert_called_once_with(json_filename)
    mock_delete_old_logfile.assert_called_once_with(log_filename)
    mock_init_logfile.assert_called_once_with(log_filename)
    mock_init_future.assert_called_once_with(json_future_filename)


def test_create_files_subsequent_run(mock_file_system, mocker):
    # Arrange
    log_filename = 'test.log'
    json_filename = 'test.json'
    json_future_filename = 'test_future.json'

    mock_path, mock_delete_old_logfile, mock_init_logfile, mock_init_future, mock_init_storage = mock_file_system
    mock_path.return_value.is_file.side_effect = [True, True]
    mocker.patch('os.path.getsize', return_value=1)  # Mock file size greater than 0

    # Act
    first_run = create_files(log_filename, json_filename, json_future_filename)

    # Assert
    assert first_run == False
    mock_init_storage.assert_not_called()
    mock_delete_old_logfile.assert_not_called()
    mock_init_logfile.assert_not_called()
    mock_init_future.assert_not_called()


def test_init_future(mocker):
    # Arrange
    mock_open = mocker.patch('builtins.open', mocker.mock_open())

    # Define the expected JSON data to be written
    expected_data = []

    # Act
    init_future('test_future.json')

    # Assert
    mock_open.assert_called_once_with('test_future.json', 'w')
    mock_open().write.assert_called_once_with(json.dumps(expected_data))


def test_init_logfile(mocker):
    # Arrange
    mock_open = mocker.patch('builtins.open', mocker.mock_open())
    mock_print = mocker.patch('builtins.print')

    log_filename = 'test.log'

    # Act
    init_logfile(log_filename)

    # Assert
    mock_open.assert_called_once_with(log_filename, 'a+')
    mock_print.assert_called_once_with("Start of log file", file=mock_open.return_value)


class TestSetUp:
    @pytest.fixture(autouse=True)
    def setup(self, mocker):
        self.mocker = mocker
        self.mock_args = mocker.MagicMock()
        self.mock_parse_args = mocker.patch('src.utils.parse_args', return_value=self.mock_args)
        self.mock_config = {
            'prod': {'json_file': 'prod.json', 'json_file_future': 'prod_future.json', 'log_file': 'prod.log'},
            'test': {'json_file': 'test.json', 'json_file_future': 'test_future.json', 'log_file': 'test.log'},
            'pytesting': {'json_file': 'pytesting.json', 'json_file_future': 'pytesting_future.json', 'log_file': 'pytesting.log'}
        }
        self.mock_read_config_file = mocker.patch('src.utils.read_config_file', return_value=self.mock_config)
        self.mock_first_run = True
        self.mock_create_files = mocker.patch('src.utils.create_files', return_value=self.mock_first_run)
        self.mock_check_cla_match_env = mocker.patch('src.utils.check_cla_match_env')
        self.mock_logging_basicConfig = mocker.patch('logging.basicConfig')

    @pytest.mark.parametrize('caff_env', ['prod', 'test', 'pytesting'])
    def test_set_up_valid_env(self, caff_env):
        # Arrange
        expected_args = ['script.py']
        if caff_env == 'test':
            expected_args = ['script.py', '-t']
        elif caff_env == 'pytesting':
            expected_args = ['script.py', '-q']
        self.mocker.patch.dict('os.environ', {'CAFF_ENV': caff_env})
        self.mocker.patch('sys.argv', expected_args)

        # Act
        log_filename, json_filename, json_future_filename, first_run, args = set_up()

        # Assert
        expected_log_filename = self.mock_config[caff_env]['log_file']
        expected_json_filename = self.mock_config[caff_env]['json_file']
        expected_json_future_filename = self.mock_config[caff_env]['json_file_future']

        self.mock_parse_args.assert_called_once_with(expected_args[1:])
        self.mock_read_config_file.assert_called_once_with('src/caffeine.ini')
        self.mock_check_cla_match_env.assert_called_once_with(caff_env, self.mock_args)
        self.mock_create_files.assert_called_once_with(expected_log_filename, expected_json_filename, expected_json_future_filename)
        self.mock_logging_basicConfig.assert_called_once_with(filename=expected_log_filename, level=logging.INFO,
                                                              format='%(levelname)s: %(message)s')
        assert log_filename == expected_log_filename
        assert json_filename == expected_json_filename
        assert json_future_filename == expected_json_future_filename
        assert first_run == self.mock_first_run
        assert args == self.mock_args

    @pytest.mark.parametrize('caff_env', ['nonsense', None, ''])
    def test_set_up_invalid_env(self, caff_env):
        # Arrange
        self.mocker.patch.dict('os.environ', clear=True)

        # Act and Assert
        with pytest.raises(SystemExit):
            set_up()```
